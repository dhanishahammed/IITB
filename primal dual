import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, mean_squared_error

iris = load_iris()
X = iris.data
y = (iris.target == 0).astype(float)

scaler = StandardScaler()
X = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

def f(beta):
  residual = X_train @ beta - y_train
  return 0.5 * np.sum(residual ** 2)

def grad_f(beta):
  residual = X_train @ beta - y_train
  return X_train.T @ residual

def phi(lambda_):
  residual = X_train @ lambda_ - y_train
  return 0.5 * np.sum(residual ** 2)

def grad_phi(lambda_):
  residual = X_train @ lambda_ - y_train
  return X_train.T @ residual

def backtracking_line_search(f, grad, x, dk, alpha=0.1, gamma=0.8):
  beta_k = 1
  while f(x + beta_k * dk) > f(x) + alpha * beta_k * grad.T @ dk:
      beta_k *= gamma
  return beta_k

def minimize_subspace(f, grad_f, y_k, i_k, tol=1e-6, max_iter=100):
  x_k = y_k.copy()
  for j in range(max_iter):
    grad = grad_f(x_k)
    dk = np.zeros_like(x_k)
    dk[i_k] = -grad[i_k]
    beta_k = backtracking_line_search(f, grad, x_k, dk)
    x_k += beta_k * dk
    if abs(grad[i_k]) < tol:
      break
  return x_k

def primal_dual_aam(f, grad_f, phi, grad_phi, beta0, v0):
  max_iter = 1000
  tol = 1e-6
  beta = beta0
  v = v0
  A = 0
  a = 1
  eta = np.zeros_like(beta)
  for k in range(max_iter):
    g_k = grad_f(beta)
    dk = v - beta
    beta_k = backtracking_line_search(f, g_k, beta, dk)
    y_k = beta + beta_k * dk
    g_lambda_k = grad_phi(eta)
    i_k = np.argmax(g_lambda_k ** 2)
    x_k = minimize_subspace(f, grad_f, y_k, i_k)
    lambda_k = beta_k * eta + (1 - beta_k) * eta
    eta_k_plus_1 = minimize_subspace(phi, grad_phi, lambda_k, i_k)
    norm_grad_squared = np.linalg.norm(g_lambda_k) ** 2
    A_k_plus_1 = A + a
    a_k_plus_1 = np.sqrt(2 * A_k_plus_1 * (f(y_k) - f(x_k)) / norm_grad_squared)
    v = v - a_k_plus_1 * g_lambda_k
    beta = x_k
    A = A_k_plus_1
    a = a_k_plus_1
    eta = eta_k_plus_1

    if np.linalg.norm(g_lambda_k) < tol:
      print(f"Converged in {k+1} iterations")
      break

  return beta, eta


beta0 = np.random.randn(X_train.shape[1])
v0 = np.zeros_like(beta0)

beta_opt, eta_opt = primal_dual_aam(f, grad_f, phi, grad_phi, beta0, v0)
y_pred = (X_test @ beta_opt > 0.5).astype(float)
accuracy = accuracy_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)

print(f"Optimal beta: {beta_opt}")
print(f"Optimal eta: {eta_opt}")
print(f"Test Accuracy: {accuracy}")
print(f"Test MSE: {mse}")
