import numpy as np

def mse_loss(theta, X, y):
  predictions = X @ theta
  return np.mean((predictions - y) ** 2)

def grad_mse_loss(theta, X, y):
  predictions = X @ theta
  return (2 / X.shape[0]) * X.T @ (predictions - y)

def S(i):
  def constraint(y):
      return y  
  return constraint

def primal_dual_aam(p, grad_p, S, x0, eta0, zeta0, lambda0):
  A0 = 0
  x_hat = x0
  eta = eta0
  zeta = zeta0
  lambda_k = lambda0
  A = A0
  a = 1e-6
  max_iter = 100
  tol = 1e-6

  for k in range(max_iter):
    beta_set = np.linspace(0, 1, 100)
    beta_values = [p(lambda_k + beta * (zeta - eta)) for beta in beta_set]
    beta_k = beta_set[np.argmin(beta_values)]
    lambda_k = beta_k * zeta + (1 - beta_k) * eta

    grad_lambda_k = grad_p(lambda_k)
    i_k = np.argmax([np.linalg.norm(grad_lambda_k[i]) ** 2 for i in range(grad_lambda_k.shape[0])])

    eta_set = [S(i_k)(lambda_k)]
    eta_values = [p(eta_candidate) for eta_candidate in eta_set]
    eta_next = eta_set[np.argmin(eta_values)]

    A_k = A + a
    a_k = a
    while not np.isclose(p(lambda_k) - (a_k ** 2) / (2 * A_k) * np.linalg.norm(grad_lambda_k) ** 2, p(eta_next), atol=tol):
      a_k *= 0.5

    zeta_next = zeta - a_k * grad_lambda_k
    x_hat_next = (a_k * lambda_k + A * x_hat) / A_k

    eta = eta_next
    zeta = zeta_next
    A = A_k
    x_hat = x_hat_next

  return x_hat, eta
