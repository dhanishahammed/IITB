import numpy as np

def mse_loss(theta, X, y):
  predictions = X @ theta
  return np.mean((predictions - y) ** 2)

def grad_mse_loss(theta, X, y):
  predictions = X @ theta
  return (2 / X.shape[0]) * X.T @ (predictions - y)

def S(i):
  def constraint(y):
    return y
  return constraint

def aam(f, grad_f, S, x0, v0):
  A0 = 0
  x = x0
  v = v0
  A = A0
  a = 1.0
  max_iter=100
  tol=1e-6

  for k in range(max_iter):
    beta_set = np.linspace(0, 1, 100)
    beta_values = [f(x + beta * (v - x)) for beta in beta_set]
    beta_k = beta_set[np.argmin(beta_values)]
    y_k = x + beta_k * (v - x)

    grad_y_k = grad_f(y_k)
    i_k = np.argmax([np.linalg.norm(grad_y_k[i]) ** 2 for i in range(grad_y_k.shape[0])])

    x_set = [S(i_k)(y_k)]
    x_values = [f(x_candidate) for x_candidate in x_set]
    x_k = x_set[np.argmin(x_values)]
    A_k = A + a
    a_k = a
    tol = 1e-6
    while not np.isclose(f(y_k) - (a_k ** 2) / (2 * A_k) * np.linalg.norm(grad_y_k) ** 2, f(x_k), atol=tol):
      a_k *= 0.5
    v = v - a_k * grad_y_k

  return x_k

np.random.seed(0)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)
b = np.c_[np.ones((100, 1)), X]
theta0 = np.random.randn(2, 1)
v0 = np.random.randn(2, 1)

result = aam(lambda theta: mse_loss(theta, b, y), lambda theta: grad_mse_loss(theta, b, y), S, theta0, v0)
print("Optimized theta:", result)
