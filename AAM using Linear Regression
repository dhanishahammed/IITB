import numpy as np

def mse_loss(theta, X, y):
  predictions = X @ theta
  return np.mean((predictions - y) ** 2)

def grad_mse_loss(theta, X, y):
  predictions = X @ theta
  return (2 / X.shape[0]) * X.T @ (predictions - y)

def S(i):
  def constraint(y):
    return y
  return constraint

def aam(f, grad_f, S, x0, v0):
  A0 = 0
  x_k = x0
  v_k = v0
  A_k = A0
  a_k = 1.0
  max_iter=100
  tol=1e-6

  for k in range(max_iter):
    beta_set = np.linspace(0, 1, 100)
    beta_values = [f(x_k + beta * (v_k - x_k)) for beta in beta_set]
    beta_k = beta_set[np.argmin(beta_values)]
    y_k = x_k + beta_k * (v_k - x_k)

    grad_y_k = grad_f(y_k)
    i_k = np.argmax([np.linalg.norm(grad_y_k[i]) ** 2 for i in range(grad_y_k.shape[0])])

    x_set = [S(i_k)(y_k)]
    x_values = [f(x_candidate) for x_candidate in x_set]
    x_next = x_set[np.argmin(x_values)]
    A_next = A_k + a_k
    a_next = a_k
    tol = 1e-6
    while not np.isclose(f(y_k) - (a_next ** 2) / (2 * A_next) * np.linalg.norm(grad_y_k) ** 2, f(x_next), atol=tol):
      a_next *= 0.5
    v_k = v_k - a_next * grad_y_k
    x_k = x_next
    A_k = A_next
    a_k = a_next
  return x_k

np.random.seed(0)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)
b = np.c_[np.ones((100, 1)), X]
theta0 = np.random.randn(2, 1)
v0 = np.random.randn(2, 1)

result = aam(lambda theta: mse_loss(theta, b, y), lambda theta: grad_mse_loss(theta, b, y), S, theta0, v0)
print("Optimized theta:", result)
